{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "## Overview\n",
    "In this exercise we will be trying to predict the electrical usage of the boole library on a particular day based on a number of factors, including the opening hours, footfall, heating degree days and number of hours of daylight.\n",
    "\n",
    "We have two datasets - for 2012 and 2013. They contain the opening hours, footfall, heating degree days and daylight hours for a number of days in each year for the Boole library. 2012 includes the electrical usage for each day, whereas 2013 does not.\n",
    "\n",
    "First, you'll investigate the 2012 data and build a correlation matrix to see what it looks like.\n",
    "\n",
    "You will then build a regression model from the 2012 data. First, you'll need to split the data into training and testing data.\n",
    "\n",
    "Next, you will need use cross validation on the training data to train a number of models with different degrees of polynomial features.\n",
    "\n",
    "The best performing feature set will then be used to build the final model on the full set of training data. This will be then evaluated on the held-out test set, to give an accurate r2 score. You can plot the predictions against the actual values to see how they look.\n",
    "\n",
    "Once you determine the model is performing well, you use it to predict electrical usage for 2013. You can then compare the two datasets and see why they differ.\n",
    "\n",
    "\n",
    "## Report\n",
    "\n",
    "You are required to produce a final report which should include the following:\n",
    "1. Introduction – Brief introduction including the objectives of the assignment and relevance of data analytics for the analysis and improvement of building energy performance.\n",
    "2. Correlation matrix for the 2012 Dataset – Comment on results and the relevance of the results achieved.\n",
    "3. Show that cross validation was performed using regression on a number of polynomial features. Give the average score across the folds for each polynomial degree.\n",
    "4. Build the final regression model using the best feature set. Include the intercept and weights for this model in your report (copy & pasting directly from the notebook is fine).\n",
    "5. Evaluate the final regression model on the test set. This may be done with a scoring metric and/or a useful plot.\n",
    "4. Show the final regression model test set score. Include a copy of the intercept & weights.\n",
    "5. Plot the measured y against the estimated y on the test set. Comment on your findings.\n",
    "6. Make a prediction for the electricity usage of the Boole library in 2013 based on footfall, daylight hours, opening hours and heating degree days. Discuss your result and compare with the electricity consumption for 2012. Discuss why there may be differences or similarities. The comparison stage of this may be done in python or Microsoft excel.\n",
    "6. Brief discussion and conclusion on the overall results achieved in the assignment\n",
    "\n",
    "\n",
    "You may find the headings and questions below useful in framing the various sections of your report. Note that answering these questions is not mandatory; they are merely prompts for discussion. Final report should come in around 10 pages.\n",
    "* Why do we separate training and testing data?\n",
    "* What is the purpose of cross validation?\n",
    "* Are our 2013 predictions accurate?\n",
    "* What are heating degree days? Why would they have/not have an effect on the electricity usage?\n",
    "* Did the model improve much from using the base set of features (i.e. polynomial degree 1) to higher degree polynomials? Why might thei be the case?\n",
    "* Check the validity/sanity of data, common sense approach, state all assumptions and rationale behind the same\n",
    "\n",
    "\n",
    "This notebook will serve as the basis for the analysis of the Boole Dataset.\n",
    "\n",
    "## Using Jupyter Notebooks\n",
    "Jupyter is a _notebook_ development environment for python. If you've used mathematica before, you'll be familiar with the notebook concept.\n",
    "\n",
    "Each cell (like this one) can either be for _code_ or _markdown_ (a very basic language for formatting text). This cell is a markdown cell.\n",
    "\n",
    "Clicking in the `typing` area of a code cell will let you enter edit mode on that cell. Pressing ctrl+Enter will run the code in that cell. Keyboard shortcuts can be found by clicking on help -> keyboard shortcuts.\n",
    "\n",
    "Please note that any variable created in the code can be viewed by typing `print(variable)`.\n",
    "\n",
    "You are allowed to copy and paste or otherwise edit some of the code in the housing notebook to apply it to your own datasets if you are not comfortable writing from scratch. As well as this, 99% of errors or anything else you are having trouble with can be solved by a quick google. In particular, the the [cross-validated](http://stats.stackexchange.com/) and [stack-overflow](http://stackoverflow.com/) stack exchange sites are great resources.\n",
    "\n",
    "### Important Note\n",
    "**NB**: Any code you will need to fill in will be marked with a comment **next** to it. OPther comments above code do not necessarily mean you need to adapt that code, they are just there to explain what's happening. E.g., in the code below, the first line does not need to be altered, but the second line you'll need to name your model.\n",
    "\n",
    "```python\n",
    "# get the training data:\n",
    "X_train = X[train_indices]\n",
    "\n",
    "# name the model\n",
    "= LinearRegression() # name the model here\n",
    "```\n",
    "\n",
    "## How to load the Jupyter notebook\n",
    "1. Open Chrome and go to https://notebooks.azure.com\n",
    "2. Sign in with your UCC IT account. If you can't remember this, but have a microsoft/hotmail account, you can sign in with this. Otherwise, you'll need to create an account.\n",
    "3. Click on \"Libraries\" in the top left\n",
    "4. Click on \"New Library\", then click on \"From GitHub\"\n",
    "5. The GitHub Repo is https://github.com/lkev/ce3010_lab\n",
    "6. Give it the name \"CE3010 Lab FirstName LastName\" (with your actual name, not FirstName LastName)\n",
    "7. Give it the ID ce3010-firstname-lastname\n",
    "8. Click Import\n",
    "\n",
    "The notebook you'll be using during the class is \"Houseing - Partially Filled\".\n",
    "\n",
    "Notebook for homework is \"Boole - Partially Filled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Libraries\n",
    "First, we must import the data and relevant libraries\n",
    "\n",
    "---\n",
    "The following code imports various python packages we need to use, and also imports the boole data.\n",
    "\n",
    "`boole_data` is the 2012 data\n",
    "\n",
    "`boole_data_2` is the 2013 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-03T11:34:22.400696Z",
     "start_time": "2017-10-03T11:34:21.453033Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display plots & graphs in browser:\n",
    "%matplotlib inline\n",
    "\n",
    "# Various libraries that are required\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# set the plot styles\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "# Import house data\n",
    "boole_data = pd.read_csv(\"Boole Library Electric Consumption 2012.csv\")\n",
    "boole_data_2 = pd.read_csv(\"Boole Library Electric Consumption 2013.csv\")\n",
    "\n",
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    pd.set_option('display.max_seq_items', len(x))\n",
    "    display(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "    pd.reset_option('display.max_seq_items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the data\n",
    "Displaying data to get a good idea of what it looks like should always be the first step of any analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:09.332331Z",
     "start_time": "2017-10-02T20:37:09.311275Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Total number of samples in data:', ) # get the len() of the data\n",
    "\n",
    "#Display first ten rows of the boole_data.\n",
    "     # get the head of the data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Correlation Matrix\n",
    "We use the numpy (np) python package to get the correlation coefficients:\n",
    "\n",
    "```python\n",
    "c = np.coeff(X)\n",
    "```\n",
    "\n",
    "Adapt the code below to display a correlation matrix.\n",
    "\n",
    "Note the `boole_data` must be transpoed (`boole_data.T`) in the `np.corrcoed()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:13.690146Z",
     "start_time": "2017-10-02T20:37:13.319269Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an array of correlation data. Include .T for transposing\n",
    "corr = np.corrcoef() # give the data you want to get coefficients for\n",
    "\n",
    "# Create the axis labels for use in the plot\n",
    "labels = # use the column names as labels\n",
    "\n",
    "# set the figure size\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot a heatmap to easily visualise the relationships\n",
    "sns.heatmap(corr, xticklabels=labels, yticklabels=labels, annot=True,\n",
    "            cmap='RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The code below can be used to plot the various correlations if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:22.824961Z",
     "start_time": "2017-10-02T20:37:22.367708Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot various variables against elec_usage\n",
    "sns.pairplot(data=, x_vars=[],    # fill the data and which x variables you want to view\n",
    "             y_vars=[], kind='scatter', size=5) # plot against the y_var elec_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data into Training and Testing Sets\n",
    "\n",
    "\n",
    "Use the code below to split the 2012 data into training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:32.054563Z",
     "start_time": "2017-10-02T20:37:32.039518Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a matrix of the features\n",
    "X = # cerate your matrix here from the boole data\n",
    "\n",
    "# Create a vector for the dependent variable/target values (elec_usage)\n",
    "y = # create your targets here from the boole data\n",
    "\n",
    "# Create an array of indices of the training and testing data. The first ~80%\n",
    "# of the data is used for training, with the rest for testing\n",
    "train_indices = np.arange(0, len(X) * 0.8)\n",
    "test_indices = np.arange(len(X) * 0.8, len(X))\n",
    "\n",
    "# shuffle the data\n",
    "X, y = shuffle(X, y)\n",
    "\n",
    "# get the training samples\n",
    "X_train = X.iloc[train_indices]\n",
    "y_train = y.iloc[train_indices]\n",
    "\n",
    "# and the test samples\n",
    "X_test = X.iloc[test_indices]\n",
    "y_test = y.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Initial Model\n",
    "\n",
    "You can use this section (section 4) to familiarise yourself with the basic training and testing process, or you can skip below to go straight to section 5 - Feature Selection & cross Validation to find the optimal model with polynomial features. **It's recommended you do this short section first to get familiar with the code.**\n",
    "\n",
    "## Training\n",
    "Adapt the code below to train a new model on the training data, `X_train` and `y_train`. This results in finding the weights, $w$.\n",
    "\n",
    "You must name your model:\n",
    "```python\n",
    "model_name = LinearRegression(fit_intercept=True)\n",
    "model_name.fit()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:36.914572Z",
     "start_time": "2017-10-02T20:37:36.905549Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an instance of the sklearn linear regression model\n",
    " = LinearRegression(fit_intercept=True) # name your model!\n",
    "\n",
    "# fit the model to the training data\n",
    ".fit() # use the model name from above, fill the training data\n",
    "\n",
    "# once again you'll need to use the model_name.intercept_ and model_name.coef_\n",
    "intercept = .intercept_ # use model name\n",
    "weights = .coef_ # use model name\n",
    "\n",
    "# the 'format' function here is just to format to 2 decimal places\n",
    "print('intercept (w_0):', intercept)\n",
    "print('weights (w_1, w_2, w_3, w_4):', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Adapt code below gets predictions, `y_hat`, using the weights stored in your trained model.\n",
    "\n",
    "You will then calculate the score using the `r2_score` function.\n",
    "\n",
    "**NOTE** `r2_score` is in the form `r2_score(y_test, y_hat)`, **not** `r2_score(y_hat, y_test)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:42.295852Z",
     "start_time": "2017-10-02T20:37:42.287858Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find predictions using the test data\n",
    "y_hat = .predict() # use model name\n",
    "\n",
    "# get the r2 score\n",
    "score = r2_score(, ) # get the r2 of y_test and y_hat\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the Predictions\n",
    "\n",
    "Below you can plot `y_hat` against `y_test`.\n",
    "\n",
    "This code does not need to be edited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:37:45.291007Z",
     "start_time": "2017-10-02T20:37:45.037333Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create vector of numbers from 1 to length of y_plot, to serve as the x-axis:\n",
    "nums = np.arange(len(y_hat))\n",
    "\n",
    "# sort the values in ascending order\n",
    "y_test_sorted = np.sort(y_test)\n",
    "y_hat_sorted = y_hat[np.argsort(y_test)]\n",
    "\n",
    "# difference between y_hat and y_test\n",
    "delta = abs(y_hat_sorted - y_test_sorted)\n",
    "\n",
    "# create the plot\n",
    "fig, ax = plt.subplots(figsize = (10,8))\n",
    "\n",
    "# Plot the delta as a line graph\n",
    "ax.plot(nums, delta, label = 'delta (|y_test - y_hat|)')\n",
    "\n",
    "# Plot y_hat\n",
    "ax.scatter(nums, y_hat_sorted, label='Predicted/Modelled Values')\n",
    "\n",
    "# plot y_test\n",
    "ax.scatter(nums, y_test_sorted, label='Actual Test Set Values')\n",
    "\n",
    "# Set up legend & title\n",
    "fig.suptitle('Plot of y_test vs. y_hat', fontsize='xx-large')\n",
    "ax.legend(fontsize='x-large', frameon=True, shadow=True)\n",
    "ax.set_xlabel('sample no.')\n",
    "ax.set_ylabel('predicted/actual heating oil usage (kWh)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection & Cross Validation\n",
    "In this section you will generate various different feature sets for all the samples in the training data.\n",
    "\n",
    "You will use cross validation to select the best feature sets.\n",
    "\n",
    "## Cross Validation\n",
    "Below, you will perform cross validation across various numbers of polynomial features.\n",
    "\n",
    "The `KFold().split()` function in `sklearn` allows us to automatically split the data across a number of folds.\n",
    "\n",
    "It is trained on the training portion of each fold, and scored on the validation portion.\n",
    "\n",
    "For each `poly_degree`, you will print the average of the score across all folds.\n",
    "\n",
    "Use the best performing number of features to build the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:47:19.691010Z",
     "start_time": "2017-10-02T20:47:19.568685Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_degrees = [] # decide on which degrees to use\n",
    "cv_folds = KFold(n_splits=) # decide how many cross validation splits you want\n",
    "\n",
    "# set up the scores dictionary\n",
    "scores = {}\n",
    "\n",
    "# loop through each set of features\n",
    "for poly_degree in poly_degrees:\n",
    "    # set up the transformer\n",
    "    poly_transformer = PolynomialFeatures(\n",
    "        degree=poly_degree, include_bias=False)\n",
    "\n",
    "    # generate the new set of features\n",
    "    X_train_poly = poly_transformer.fit_transform() # generate new features from\n",
    "                                                    # the base training set of X\n",
    "    \n",
    "    # empty scores list\n",
    "    scores[poly_degree] = []\n",
    "    \n",
    "    print('\\nscores for poly_degree={}:'.format(poly_degree))\n",
    "    \n",
    "    for i, (train_indices, validation_indices) in enumerate(\n",
    "            cv_folds.split(X_train, y_train)):\n",
    "        \n",
    "        # get the X and y train and validation set for this fold\n",
    "        X_train_cv = X_train_poly[] # use the training indices for this split\n",
    "        y_train_cv = y_train.iloc[] # use the training indices\n",
    "        \n",
    "        X_valid_cv = X_train_poly[] # use the validation indices for this split\n",
    "        y_valid_cv = y_train.iloc[] # use the validation indices\n",
    "        \n",
    "        # build the model and make predictions\n",
    "         = LinearRegression(fit_intercept=True) # name the model\n",
    "        .fit(, ) # fit the model to your training data\n",
    "        \n",
    "        y_hat = .predict() # generate predictions using your\n",
    "                           # validation set for this fold\n",
    "        \n",
    "        # score it\n",
    "        fold_score = r2_score(, ) # score for validation y and your estimate\n",
    "        \n",
    "        # store the score\n",
    "        scores[poly_degree].append(fold_score)\n",
    "        \n",
    "        print('fold {}: {:.2f}'.format(i, fold_score))\n",
    "    \n",
    "    average_score = np.mean(scores[poly_degree])\n",
    "    print('average across folds: {:.2f}'.format(average_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Model on Best Feature Set\n",
    "\n",
    "Now, use the best performing polynomial degree feature set from above to generate your final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:47:21.760731Z",
     "start_time": "2017-10-02T20:47:21.743688Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_transformer = PolynomialFeatures(degree=, include_bias=False) # set your polynomial degree\n",
    "# generate the new set of features\n",
    "X_train_poly = poly_transformer.fit_transform() # generate your new training\n",
    "                                                # features using this polynomial degree\n",
    "X_test_poly = poly_transformer.transform() # generate your new testing features\n",
    "                                           # using this polynomial degree\n",
    "\n",
    " = LinearRegression(fit_intercept=True) # name your final model\n",
    ".fit(, ) # fit using your training data\n",
    "\n",
    "y_hat = .predict() # make estimations on the testing data\n",
    "\n",
    "score = r2_score(, ) # generate a final r2 score between estimation and actual\n",
    "                     # responses in the test set \n",
    "\n",
    "print('final score on test set:', score)\n",
    "\n",
    "print('intercept:\\n',  .intercept_) # print the intercept from the model\n",
    "print('weights:\\n', .coef_) # print the other weights from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional:\n",
    "\n",
    "You can generate a new graph using the plotting code from above to compare the two visually and see if any improvements have been made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making future predictions (for 2013)\n",
    "If the r2 score is high, we can use the final model from earlier to make predictions from the 2013 dataset.\n",
    "\n",
    "You can use the code below to view the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:47:24.249467Z",
     "start_time": "2017-10-02T20:47:24.238463Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "house_data_2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input into existing model\n",
    "\n",
    "Use your final model to make predictions on the new data.\n",
    "\n",
    "Note that you must first transform the features with a 3rd-degree polynomial, as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:47:26.499499Z",
     "start_time": "2017-10-02T20:47:26.493479Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new matrix of features\n",
    "X_new = # get the data from the new  dataset\n",
    "\n",
    "X_new_poly = poly_transformer.transform() # transform the features using your\n",
    "                                          # transformer from before (it should still be saved)\n",
    "\n",
    "# Create a vector of actual target values\n",
    "y_pred_new = final_oil_use_model.predict() # make predictions using the new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Predictions against 2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've now got predictions for all the new Data. We can save these predictions as a csv for pasting into excel, or manipulate them from within python.\n",
    "\n",
    "You can sum up and compare the total consumption from each dataset, and infer why there might be differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-02T20:47:28.575409Z",
     "start_time": "2017-10-02T20:47:28.551340Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Can export y_pred_new for pasting into the existing Household Dataset1 csv,\n",
    "# for analysis in excel\n",
    "np.savetxt('house_dataset2_predictions.csv', y_pred_new)\n",
    "\n",
    "# Here, we'll perform analysis in python\n",
    "# First, we get the average of all features in dataset1 and _new\n",
    "# The (0) in .mean(0) means go along axis 0, i.e. the columns, instead of axis\n",
    "# 1, i.e. the rows\n",
    "print('means for Dataset1: \\n', house_data.mean(0), '\\n')\n",
    "print('means for Dataset2: \\n', house_data_2.mean(0), '\\n',\n",
    "      'estimated_oil_usage', np.mean(y_pred_new))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nav_menu": {},
  "notify_time": "10",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "386px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": "3",
   "toc_cell": false,
   "toc_position": {
    "height": "271px",
    "left": "1663px",
    "right": "20px",
    "top": "182px",
    "width": "178px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  },
  "toc_position": {
   "height": "1081px",
   "left": "0px",
   "right": "1948px",
   "top": "107px",
   "width": "186px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
